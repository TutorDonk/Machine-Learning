{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SdIVICH7ESWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64179836-330a-449f-b38f-79096f309fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q tensorflow-recommenders\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Library and Load Dataset"
      ],
      "metadata": {
        "id": "uwEy5hOFku6H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KiBNn_qirtF1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_recommenders as tfrs\n",
        "from typing import Dict, Text\n",
        "import numpy as np\n",
        "df_sma = pd.read_csv('/content/dataset_sma.csv')\n",
        "df_sd = pd.read_csv('/content/dataset_sd.csv')\n",
        "df_smp = pd.read_csv('/content/dataset_smp.csv')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "ytA8USjrlBF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_merged = pd.concat([df_sma, df_sd, df_smp])\n",
        "\n",
        "df_merged['gender_tutor'], gender_unique = pd.factorize(df_merged['gender_tutor'])\n",
        "df_merged['pelajaran'], pelajaran_unique = pd.factorize(df_merged['pelajaran'])\n",
        "df_merged['daerah_tutor'], daerah_unique = pd.factorize(df_merged['daerah_tutor'])\n",
        "df_merged.to_csv('/content/merged_dataset.csv', index=False)\n",
        "print(df_merged.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7MlPJbdlAYv",
        "outputId": "85f5836c-a116-469d-c95e-ed9eda8fe1a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 3000 entries, 0 to 999\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype\n",
            "---  ------         --------------  -----\n",
            " 0   id_user        3000 non-null   int64\n",
            " 1   id_tutor       3000 non-null   int64\n",
            " 2   jenjang_tutor  3000 non-null   int64\n",
            " 3   gender_tutor   3000 non-null   int64\n",
            " 4   daerah_tutor   3000 non-null   int64\n",
            " 5   pelajaran      3000 non-null   int64\n",
            "dtypes: int64(6)\n",
            "memory usage: 164.1 KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LSHRC9ohCpCo"
      },
      "outputs": [],
      "source": [
        "lookup_daerah = tf.keras.layers.IntegerLookup()\n",
        "lookup_pelajaran = tf.keras.layers.IntegerLookup()\n",
        "lookup_jenjang = tf.keras.layers.IntegerLookup()\n",
        "lookup_gender = tf.keras.layers.IntegerLookup()\n",
        "lookup_tutor = tf.keras.layers.IntegerLookup()\n",
        "lookup_user = tf.keras.layers.IntegerLookup()\n",
        "\n",
        "daerah_ds = tf.data.Dataset.from_tensor_slices(df_merged['daerah_tutor'])\n",
        "pelajaran_ds = tf.data.Dataset.from_tensor_slices(df_merged['pelajaran'])\n",
        "jenjang_ds = tf.data.Dataset.from_tensor_slices(df_merged['jenjang_tutor'])\n",
        "gender_ds = tf.data.Dataset.from_tensor_slices(df_merged['gender_tutor'])\n",
        "tutor_ds = tf.data.Dataset.from_tensor_slices(df_merged['id_tutor'])\n",
        "user_ds = tf.data.Dataset.from_tensor_slices(df_merged['id_user'])\n",
        "\n",
        "lookup_daerah.adapt(daerah_ds)\n",
        "lookup_pelajaran.adapt(pelajaran_ds)\n",
        "lookup_jenjang.adapt(jenjang_ds)\n",
        "lookup_gender.adapt(gender_ds)\n",
        "lookup_tutor.adapt(tutor_ds)\n",
        "lookup_user.adapt(user_ds)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model and Train"
      ],
      "metadata": {
        "id": "Ucz7BMhKlQse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kh9nj6fTCsFy"
      },
      "outputs": [],
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "    def __init__(self, user_vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.user_embedding = tf.keras.layers.Embedding(user_vocab_size, embedding_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.user_embedding(inputs)\n",
        "        return self.dense(x)\n",
        "\n",
        "class TutorModel(tf.keras.Model):\n",
        "    def __init__(self, tutor_vocab_size, daerah_vocab_size, gender_vocab_size, jenjang_vocab_size, pelajaran_vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.daerah_embedding = tf.keras.layers.Embedding(daerah_vocab_size, embedding_dim)\n",
        "        self.gender_embedding = tf.keras.layers.Embedding(gender_vocab_size, embedding_dim)\n",
        "        self.jenjang_embedding = tf.keras.layers.Embedding(jenjang_vocab_size, embedding_dim)\n",
        "        self.pelajaran_embedding = tf.keras.layers.Embedding(pelajaran_vocab_size, embedding_dim)\n",
        "        self.tutor_embedding = tf.keras.layers.Embedding(tutor_vocab_size, embedding_dim)\n",
        "        self.dense = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        daerah_input, gender_input, jenjang_input, pelajaran_input, tutor_input = inputs\n",
        "        daerah_emb = self.daerah_embedding(daerah_input)\n",
        "        gender_emb = self.gender_embedding(gender_input)\n",
        "        jenjang_emb = self.jenjang_embedding(jenjang_input)\n",
        "        pelajaran_emb = self.pelajaran_embedding(pelajaran_input)\n",
        "        tutor_emb = self.tutor_embedding(tutor_input)\n",
        "        combined_emb = tf.concat([daerah_emb, gender_emb, jenjang_emb, pelajaran_emb, tutor_emb], axis=1)\n",
        "        return self.dense(combined_emb)\n",
        "\n",
        "class Model(tfrs.Model):\n",
        "    def __init__(self, user_model: tf.keras.Model, tutor_model: tf.keras.Model, task: tfrs.tasks.Retrieval):\n",
        "        super().__init__()\n",
        "        self.user_model = user_model\n",
        "        self.tutor_model = tutor_model\n",
        "        self.task = task\n",
        "\n",
        "    def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "        user_embeddings = self.user_model(features['id_user'])\n",
        "        tutor_embeddings = self.tutor_model((features[\"daerah_tutor\"], features[\"gender_tutor\"], features[\"jenjang_tutor\"], features[\"pelajaran\"], features['id_tutor']))\n",
        "        return self.task(user_embeddings, tutor_embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xvldFSYCvv-",
        "outputId": "3386f20a-6194-461e-c411-bbb5c1e8203f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "13\n",
            "3\n",
            "13\n",
            "151\n",
            "399\n",
            "Epoch 1/5\n",
            "24/24 [==============================] - 9s 209ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0110 - factorized_top_k/top_5_categorical_accuracy: 0.0127 - factorized_top_k/top_10_categorical_accuracy: 0.0153 - factorized_top_k/top_50_categorical_accuracy: 0.0340 - factorized_top_k/top_100_categorical_accuracy: 0.0577 - loss: 590.4431 - regularization_loss: 0.0000e+00 - total_loss: 590.4431\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 3s 120ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0153 - factorized_top_k/top_5_categorical_accuracy: 0.0177 - factorized_top_k/top_10_categorical_accuracy: 0.0220 - factorized_top_k/top_50_categorical_accuracy: 0.0523 - factorized_top_k/top_100_categorical_accuracy: 0.0827 - loss: 585.6753 - regularization_loss: 0.0000e+00 - total_loss: 585.6753\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 3s 129ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0103 - factorized_top_k/top_5_categorical_accuracy: 0.0133 - factorized_top_k/top_10_categorical_accuracy: 0.0183 - factorized_top_k/top_50_categorical_accuracy: 0.0573 - factorized_top_k/top_100_categorical_accuracy: 0.1040 - loss: 570.8267 - regularization_loss: 0.0000e+00 - total_loss: 570.8267\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 3s 120ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0043 - factorized_top_k/top_5_categorical_accuracy: 0.0067 - factorized_top_k/top_10_categorical_accuracy: 0.0113 - factorized_top_k/top_50_categorical_accuracy: 0.0753 - factorized_top_k/top_100_categorical_accuracy: 0.1390 - loss: 550.7591 - regularization_loss: 0.0000e+00 - total_loss: 550.7591\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 4s 170ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0047 - factorized_top_k/top_5_categorical_accuracy: 0.0107 - factorized_top_k/top_10_categorical_accuracy: 0.0230 - factorized_top_k/top_50_categorical_accuracy: 0.1113 - factorized_top_k/top_100_categorical_accuracy: 0.1823 - loss: 529.1976 - regularization_loss: 0.0000e+00 - total_loss: 529.1976\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e564057d6f0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "embedding_dim = 32\n",
        "\n",
        "daerah_vocab_size = len(lookup_daerah.get_vocabulary())\n",
        "pelajaran_vocab_size = len(lookup_pelajaran.get_vocabulary())\n",
        "gender_vocab_size = len(lookup_gender.get_vocabulary())\n",
        "jenjang_vocab_size = len(lookup_jenjang.get_vocabulary())\n",
        "tutor_vocab_size = len(lookup_tutor.get_vocabulary())\n",
        "user_vocab_size = len(lookup_user.get_vocabulary())\n",
        "\n",
        "print(daerah_vocab_size)\n",
        "print(pelajaran_vocab_size)\n",
        "print(gender_vocab_size)\n",
        "print(jenjang_vocab_size)\n",
        "print(tutor_vocab_size)\n",
        "print(user_vocab_size)\n",
        "user_model = UserModel(user_vocab_size, embedding_dim)\n",
        "tutor_model = TutorModel(tutor_vocab_size, daerah_vocab_size, gender_vocab_size, jenjang_vocab_size, pelajaran_vocab_size, embedding_dim)\n",
        "\n",
        "data = {\n",
        "    \"id_user\": lookup_user(tf.convert_to_tensor(df_merged['id_user'])),\n",
        "    \"id_tutor\": lookup_tutor(tf.convert_to_tensor(df_merged['id_tutor'])),\n",
        "    \"daerah_tutor\": lookup_daerah(tf.convert_to_tensor(df_merged['daerah_tutor'])),\n",
        "    \"gender_tutor\": lookup_gender(tf.convert_to_tensor(df_merged['gender_tutor'])),\n",
        "    \"jenjang_tutor\": lookup_jenjang(tf.convert_to_tensor(df_merged['jenjang_tutor'])),\n",
        "    \"pelajaran\": lookup_pelajaran(tf.convert_to_tensor(df_merged['pelajaran']))\n",
        "}\n",
        "\n",
        "batched_ds = tf.data.Dataset.from_tensor_slices(data).batch(128)\n",
        "\n",
        "task = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n",
        "    candidates=batched_ds.map(lambda x: tutor_model((x[\"daerah_tutor\"], x[\"gender_tutor\"], x[\"jenjang_tutor\"], x[\"pelajaran\"], x[\"id_tutor\"])))\n",
        "))\n",
        "\n",
        "model = Model(user_model, tutor_model, task)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))\n",
        "\n",
        "model.fit(batched_ds, epochs=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Recommendation Tutor for User\n"
      ],
      "metadata": {
        "id": "gw6e-yOVpaYV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QW9p4vAQE2my",
        "outputId": "6c00567c-797c-4f70-eacf-514e1b168c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top K Tutors: [108  98 106  88  73]\n",
            "Top K Scores: [3.720426  3.5097637 3.1405838 2.5417233 2.5118084]\n"
          ]
        }
      ],
      "source": [
        "def get_top_k_recommendations(model, user_id, k=10):\n",
        "    user_input = tf.convert_to_tensor([user_id], dtype=tf.int64)\n",
        "    user_input_transformed = lookup_user(user_input)\n",
        "\n",
        "    user_embedding = model.user_model(user_input_transformed)\n",
        "\n",
        "\n",
        "    tutor_ids = df_merged['id_tutor'].unique()\n",
        "    daerah_tutors = df_merged[['id_tutor', 'daerah_tutor']].drop_duplicates(subset=['id_tutor']).set_index('id_tutor').loc[tutor_ids].values.flatten()\n",
        "    gender_tutors = df_merged[['id_tutor', 'gender_tutor']].drop_duplicates(subset=['id_tutor']).set_index('id_tutor').loc[tutor_ids].values.flatten()\n",
        "    jenjang_tutors = df_merged[['id_tutor', 'jenjang_tutor']].drop_duplicates(subset=['id_tutor']).set_index('id_tutor').loc[tutor_ids].values.flatten()\n",
        "    pelajaran_tutors = df_merged[['id_tutor', 'pelajaran']].drop_duplicates(subset=['id_tutor']).set_index('id_tutor').loc[tutor_ids].values.flatten()\n",
        "\n",
        "    daerah_tutors_transformed = lookup_daerah(tf.convert_to_tensor(daerah_tutors, dtype=tf.int64))\n",
        "    gender_tutors_transformed = lookup_gender(tf.convert_to_tensor(gender_tutors, dtype=tf.int64))\n",
        "    jenjang_tutors_transformed = lookup_jenjang(tf.convert_to_tensor(jenjang_tutors, dtype=tf.int64))\n",
        "    pelajaran_tutors_transformed = lookup_pelajaran(tf.convert_to_tensor(pelajaran_tutors, dtype=tf.int64))\n",
        "    tutor_ids_transformed = lookup_tutor(tf.convert_to_tensor(tutor_ids, dtype=tf.int64))\n",
        "\n",
        "\n",
        "    tutor_embeddings = model.tutor_model((daerah_tutors_transformed, gender_tutors_transformed, jenjang_tutors_transformed, pelajaran_tutors_transformed, tutor_ids_transformed))\n",
        "\n",
        "    scores = tf.linalg.matmul(user_embedding, tutor_embeddings, transpose_b=True)\n",
        "\n",
        "    top_k_scores, top_k_indices = tf.nn.top_k(scores, k=k)\n",
        "\n",
        "    top_k_tutors = tf.gather(tutor_ids, top_k_indices[0])\n",
        "    top_k_scores = top_k_scores.numpy()[0]\n",
        "\n",
        "    return top_k_tutors.numpy(), top_k_scores\n",
        "\n",
        "\n",
        "user_id = 1\n",
        "top_k_tutors, top_k_scores = get_top_k_recommendations(model, user_id, k=5)\n",
        "\n",
        "print(\"Top K Tutors:\", top_k_tutors)\n",
        "print(\"Top K Scores:\", top_k_scores)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Model"
      ],
      "metadata": {
        "id": "54cjNe97plvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the weights in .h5 format\n",
        "user_model.save_weights('user_model_weights', save_format='keras')\n",
        "tutor_model.save_weights('tutor_model_weights', save_format='keras')\n",
        "\n",
        "# Create a new model instance and load the saved weights into it\n",
        "new_user_model = UserModel(user_vocab_size, embedding_dim)\n",
        "new_tutor_model = TutorModel(tutor_vocab_size, daerah_vocab_size, gender_vocab_size, jenjang_vocab_size, pelajaran_vocab_size, embedding_dim)\n",
        "new_task = tfrs.tasks.Retrieval(...)\n",
        "new_model = Model(new_user_model, new_tutor_model, new_task)\n",
        "\n",
        "\n",
        "model.fit(batched_ds, epochs=5)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_Lxu-JrUJRg",
        "outputId": "05982e1b-2116-4ac4-b3ec-93d622c24555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 4s 177ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0053 - factorized_top_k/top_5_categorical_accuracy: 0.0127 - factorized_top_k/top_10_categorical_accuracy: 0.0253 - factorized_top_k/top_50_categorical_accuracy: 0.1333 - factorized_top_k/top_100_categorical_accuracy: 0.2253 - loss: 509.8067 - regularization_loss: 0.0000e+00 - total_loss: 509.8067\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 5s 219ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0050 - factorized_top_k/top_5_categorical_accuracy: 0.0143 - factorized_top_k/top_10_categorical_accuracy: 0.0297 - factorized_top_k/top_50_categorical_accuracy: 0.1520 - factorized_top_k/top_100_categorical_accuracy: 0.2593 - loss: 493.3254 - regularization_loss: 0.0000e+00 - total_loss: 493.3254\n",
            "Epoch 3/5\n",
            "17/24 [====================>.........] - ETA: 1s - factorized_top_k/top_1_categorical_accuracy: 0.0083 - factorized_top_k/top_5_categorical_accuracy: 0.0230 - factorized_top_k/top_10_categorical_accuracy: 0.0437 - factorized_top_k/top_50_categorical_accuracy: 0.2183 - factorized_top_k/top_100_categorical_accuracy: 0.3529 - loss: 490.5701 - regularization_loss: 0.0000e+00 - total_loss: 490.5701"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(new_model, './ml/1')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmRwlh1VlSFh",
        "outputId": "b87c3d2b-fd5e-4eaf-d2a5-2a39190267d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.Model object at 0x7e564e131960>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.UserModel object at 0x7e56572aec50>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.TutorModel object at 0x7e564e112fe0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow_recommenders.tasks.retrieval.Retrieval object at 0x7e564e133100>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.embedding.Embedding object at 0x7e56e3757220>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.dense.Dense object at 0x7e56571811b0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.embedding.Embedding object at 0x7e564e133e80>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.embedding.Embedding object at 0x7e564e1336a0>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.embedding.Embedding object at 0x7e564e133c70>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.embedding.Embedding object at 0x7e562c0f7d00>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.embedding.Embedding object at 0x7e562c0f4c10>, because it is not built.\n",
            "WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.core.dense.Dense object at 0x7e562c0f4d00>, because it is not built.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}